{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115c5d13",
   "metadata": {},
   "source": [
    "# Create Titles for News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08cbc8",
   "metadata": {},
   "source": [
    "In this jupyter notebook we apply a pre-trained Large Language Model (Flan-T5: Flan Text-To-Text Transfer Transformer) to learn to generate titles for news articles. We apply Flan-T5 (instead of e.g. BERT) since Flan-T5 is more applicable for generating text compared to e.g. BERT. And Flan-T5 is an improvement of T5.\n",
    "\n",
    "As input we use the Dutch-news-articles data set from Kaggle that contains 255.524 news articles, with title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda24eb",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1f74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e379dc",
   "metadata": {},
   "source": [
    "## Read the Data\n",
    "The data set is the Dutch-news-articles dataset that can be found on Kaggle (https://www.kaggle.com/datasets/maxscheijen/dutch-news-articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefe4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = pd.read_csv('/data/bb8-storage/LLM-project/datasets/dutch-news-articles.csv')\n",
    "# df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])\n",
    "# df_raw.to_pickle('/data/bb8-storage/LLM-project/datasets/dutch-news-articles.pkl')\n",
    "df_raw = pd.read_pickle('/data/bb8-storage/LLM-project/datasets/dutch-news-articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65930f",
   "metadata": {},
   "source": [
    "Now split the data in 75% train cases and 25% test cases. We also convert the data from a pandas dataframe to a dataset-object as required by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337cbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_raw['content'], df_raw['title'], test_size=0.25, random_state=1234)\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'article': X_train, 'title': y_train}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'article': X_test, 'title': y_test}))\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aea99a",
   "metadata": {},
   "source": [
    "Now we tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer of T5:\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\n",
    "\n",
    "# tokenizer-function:\n",
    "def tokenize_function_t5(examples):\n",
    "    input_encodings = tokenizer(examples['article'], padding='max_length', truncation=True, max_length=512)\n",
    "    target_encodings = tokenizer(examples['title'], padding='max_length', truncation=True, max_length=32)\n",
    "    labels = target_encodings['input_ids']\n",
    "    input_encodings['labels'] = labels\n",
    "    return input_encodings\n",
    "\n",
    "# apply the tokenizer-function. Since this costs some resources, we only do this once and save the data.\n",
    "# Next time, we just load the tokenized data from disk.\n",
    "# tokenized_datasets = datasets.map(tokenize_function_t5, batched=True)\n",
    "# tokenized_datasets.save_to_disk(\"/data/bb8-storage/LLM-project/datasets/tokenized_datasets-flanT5\")\n",
    "tokenized_datasets = load_from_disk(\"/data/bb8-storage/LLM-project/datasets/tokenized_datasets-flanT5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bb94e",
   "metadata": {},
   "source": [
    "## Set up the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44e342",
   "metadata": {},
   "source": [
    "Below we load the Flan-T5-small model and set the training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c647cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpijnenbur/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\n",
    "model = model.to(device)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/data/bb8-storage/LLM-project/datasets/results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, # looks good for GPU: RTX 2080\n",
    "    per_device_eval_batch_size=16, # looks good for GPU: RTX 2080\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16 = True # added since we are using the RTX 2080\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f38f45",
   "metadata": {},
   "source": [
    "Now we prepare to collate the data and we prepare the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883b8998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392af53",
   "metadata": {},
   "source": [
    "## Train the LLM\n",
    "Now the training starts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31747443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35934' max='35934' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35934/35934 2:00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35934, training_loss=0.0, metrics={'train_runtime': 7201.6076, 'train_samples_per_second': 79.833, 'train_steps_per_second': 4.99, 'total_flos': 1.0687384197896602e+17, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1d7e1",
   "metadata": {},
   "source": [
    "The training consumes quite some resources. For this reason we save the model in the now coming code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba47ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# trainer.save_model(\"/data/bb8-storage/LLM-project/datasets/model-flan\")\n",
    "# tokenizer.save_pretrained(\"/data/bb8-storage/LLM-project/datasets/tokenizer-flan\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/data/bb8-storage/LLM-project/datasets/model-flan\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/data/bb8-storage/LLM-project/datasets/tokenizer-flan\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da616ee",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c20c02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hij had er niks mee te maken. Dat was de belangrijkste boodschap van Willem Holleeder tijdens de eerste fase van zijn proces. Vijf zittingsdagen lang was hij aan het woord. Vanaf volgende week maandag staan de verhoren van zijn zussen gepland. Holleeder zegt dat hem allerlei zaken in de schoenen worden geschoven. En daarbij is zijn zus Astrid - die volgens hem uit is op geld - de kwade genius. Volgens hem heeft zij de boel aan elkaar gelogen en de andere vrouwen tegen hem opgezet, met het doel om hem achter de tralies te krijgen. Ze vertelde \"vieze, smerige leugens\", zegt Holleeder. In dit eerste deel van het proces werd Holleeder ondervraagd aan de hand van zijn eigen handgeschreven verklaring van 127 bladzijden. Hij blijkt het dossier heel goed te kennen. Hij weet precies wat er staat, vult dat aan en gebruikt het om zijn punt te maken. Af en toe zijn er kritische vragen van de officieren van justitie, die proberen hem op tegenstrijdigheden te betrappen. Holleeder reageert dan steeds als door een wesp gestoken. \"U maakt overal zo'n punt van.\" Of als de officier opmerkt dat Holleeder altijd anderen de schuld geeft: \"ik begin een beetje flauw te worden hiervan, ze proberen me af te schilderen als slechterik.\" Hij dreigt geen antwoord meer te geven, maar zover komt het niet. Vanmiddag kreeg hij het een stuk moeilijker. De rechtbank begon met het bespreken van de gesprekken die de zussen opnamen. Daarop is te horen hoe hij tegen ze tekeer ging, en hoe hij ze bedreigde. \"Zij schreeuwen ook\", antwoordt Holleeder. \"We komen uit hetzelfde nest, zo zijn we gebekt.\" Op de tapes is te horen hoe Holleeder met van alles dreigt, van tanden uit de mond slaan tot \"à la minute doodschieten\". Daarover zegt hij nu: \"ik riep maar wat, als ik boos ben dan zeg ik dat soort dingen\". Dat moeten we niet te letterlijk nemen. Hij is er niet trots op, het hoort niet, zegt hij nu. \"Maar ik ben ook maar een simpele jongen uit de Jordaan.\" Holleeder staat niet terecht voor de bedreigingen, maar voor moordopdrachten. Ook die zouden op de tapes besproken worden. Zoals in het zinnetje \"Je weet hoe ik ben, voordat je het weet ligt hij op de grond.\" Dan had hij het niet over de liquidaties, zegt Holleeder, maar over een paar klappen geven. En wat bedoelde hij toen hij zei \"hij krijgt de kogel\", vroeg de rechtbank. \"Op dat moment had ik weer wat nieuws verzonnen om druk te zetten.\" En ook toen hij zei \"ik dreig niet, ik doe\" had dat niets met liquidaties te maken, zegt Holleeder. Ondertussen wordt het `dagje Holleeder' steeds populairder. De rij voor de Bunker was vanochtend langer dan ooit. Dagjesmensen komen vaak van ver, en sommige zijn er heel vroeg om een plek te bemachtigen. Ze zullen lang niet alles wat er besproken wordt begrijpen. Er komen heel veel namen en gebeurtenissen uit de onderwereld voorbij, zonder de context. Maar als je volhoudt, wordt je wel getrakteerd op allerlei wetenswaardigheden over het dagelijks leven van criminelen. Bellen deed Holleeder liever niet. Hij heeft een hekel aan telefoons, zegt hij. Hij liet zich liever oppiepen met een semafoon. Zo kunnen er geen gesprekken worden afgeluisterd. Maar dat is niet de uitleg die hij er zelf bij geeft. \"Dat was lekker rustig\", zegt hij. \"Mijn vriendinnen hebben allemaal een aparte pieper.\" Want \"als ze allemaal dezelfde pieper hebben gaat hij de hele dag af.\" Waar leefde Holleeder van, na zijn laatste gevangenisstraf, wilden de rechters weten. Hij vertelt dat hij 2000 euro verdiende voor iedere column in de Nieuwe Revu. En over de eerste foto na zijn vrijlating had hij een deal met de fotograaf. De opbrengst, \"tien ruggen\", zouden ze delen. Geld was er altijd wel genoeg. Zwart geld dan. Hij had 1,4 miljoen euro in cash liggen bij een advocaat. Holleeder at nooit thuis. Hij ging altijd naar restaurants, zoals Le Garage in Amsterdam en bij Herman den Blijker in Rotterdam. Thuis in de koelkast had hij alleen wat kaas liggen voor als hij laat thuis kwam. Criminelen brengen hun dagen door met samen koffie drinken en broodjes eten, zegt hij. \"Kijk, die mensen werken maar heel weinig.\" Vaste prik was zondagavond in de Bulldog. \"Daar zat iedereen.\" \"Maar het is best saai hoor, steeds een broodje eten en een drankje doen\", zegt Holleeder. Hij probeerde daarom wel aan het werk te komen. Zo wilde hij uit de onderwereld los komen, zegt hij. En op dat punt heeft de rechtbank de meeste kritische vragen. Waarom ging hij dan nog steeds om met allerlei criminelen? Waarom brak hij niet met ze? Holleeders verweer: \"ik ken veel mensen, anders kon ik niemand meer begroeten.\" Een indrukwekkende lijst criminele kennissen passeert de revue. Veel mensen leerde hij kennen in de gevangenis. Daarna hield hij contact. Maar hij deed met bijna niemand zaken. Het was \"gewoon, gezellig\". Holleeder: \"Ik ben ook niet de netste jongen. Ik ben een grote boef. Maar dat betekent niet dat ik opdracht gaf voor liquidaties.\" \n",
      "0 Holleeder vijf dagen verhoord: 'U maakt overal zo'n punt van'\n",
      "0 'I'm not a man.'\n",
      "1 Dit weekend zijn vijf UberPOP-chauffeurs betrapt door de Inspectie Leefomgeving en Transport; vier in Amsterdam en één in Rotterdam. Uber, het bedrijf achter de dienst, heeft daarom een dwangsom gekregen van 50.000 euro: 10.000 euro per betrapte chauffeur. Via UberPOP rijden particulieren zonder vergunning klanten rond voor minder dan de helft van de prijs van een taxi. De betrapte chauffeurs zelf moeten 10.000 euro betalen als ze nog eens gepakt worden bij een inspectie. Ze krijgen ook een boete, het OM moet nog bepalen hoe hoog die wordt. Het zijn niet de eerste dwangsommen en boetes voor Uber en UberPOP-chauffeurs. In december bepaalde de rechter dat de Inspectie in zijn recht staat om UberPOP aan te pakken. Amsterdamse taxichauffeurs wilden afgelopen nacht bij de ambtswoning van de burgemeester demonstreren tegen Uber. Nadat de plannen waren uitgelekt ging die demonstratie niet door.\n",
      "1 Nieuwe dwangsom voor Uber: 50.000 euro\n",
      "1 De plannen waren uitgelekt ging die demonstratie niet door.\n",
      "2 Ruim 6 jaar na hun laatste optreden heeft de Spice Girls een nieuwe tour aangekondigd. Ginger, Baby, Scary en Sporty doen mee aan de reünie, Posh zegt vooralsnog af. Volgend jaar juni staan de Britse zangeressen weer op het podium in het Verenigd Koninkrijk. Nederlandse fans zullen ervoor op reis moeten; er staan geen shows in het buitenland op de agenda. De band maakte het nieuws vanmiddag bekend op Facebook en Twitter. De meidengroep scoorde in de jaren 90 wereldwijd meerdere nummer 1-hits, zoals Wannabe . De groep verkocht tientallen miljoenen albums en maakte de term girl power populair. \"Complete bazinnen waren het\", zegt voormalig 3FM-dj Roosmarijn Reijmer in Nieuws en Co op NPO Radio 1. \"De band mag dan wel ooit ontstaan zijn als marketingconcept. Maar het is wel een girl group die de wereld heeft gedomineerd. Dat vond ik mateloos knap. Dat wilde ik ook wel worden, zo'n bazin.\" Eerder dit jaar hintten de vrouwen er al dat op een nieuwe reünie in de maak was. Victoria Beckham zag uiteindelijk af van deelname aan de nieuwe tournee, vanwege de verantwoordelijkheden voor haar modebedrijf, zegt de band. \"Die hoeft helemaal niet meer mee te doen\", zegt Reijmer over Beckhams besluit. Voor een aantal andere bandleden ligt dat anders volgens haar. \"Het is dubbel verhaal. Een paar Spice Girls kunnen echt wel wat geld gebruiken. Maar ik denk vooral dat ze er zin in hebben.\" In 2000 ging de succesvolle groep uit elkaar. Daarna was er in 2007-2008 een reünietour. In 2012 stonden alle zangeressen samen voor het laatst op het podium, tijdens de Olympische Spelen in Londen.\n",
      "2 'Complete bazinnen', Spice Girls weer op tour, maar Posh blijft thuis\n",
      "2 Spice Girls een nieuwe tour aangekondigd. Ginger, Baby, Scary and Sporty doen mee a\n"
     ]
    }
   ],
   "source": [
    "for index in range(3):\n",
    "    artikel = datasets['test'][index]['article']\n",
    "    titel = datasets['test'][index]['title']\n",
    "\n",
    "    inputs = tokenizer(artikel, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=32, num_beams=5, early_stopping=True)\n",
    "    generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(index, artikel)\n",
    "    print(index, titel)\n",
    "    print(index, generated_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
